<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<HTML xmlns="http://www.w3.org/1999/xhtml">
    <HEAD>
        <META http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <TITLE>HUBEDA</TITLE>
        <LINK rel="StyleSheet" href="./webstyle.css" type="text/css" media="all">
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Lato:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap" rel="stylesheet">
        <script src="jquery-3.2.1.js"></script>
    </HEAD>
    <BODY style="margin: 0px;">
        <div class="header">
            <div class="header-bar">
                <div class="navTable">
                    <div class="navRow" id="myNavRow">                    
                        <div style="display:table-cell;" class="button" data-path="theabout">&nbsp;&nbsp;ABOUT&nbsp;&nbsp;</div>&nbsp;&nbsp;
                        <div style="display:table-cell;"  class="button" data-path="schedule">&nbsp;&nbsp;SCHEDULE &nbsp;&nbsp;</div>                        
                        <div style="display:table-cell;" class="button" data-path="theinvited">&nbsp;&nbsp;INVITED&nbsp;SPEAKERS&nbsp;&nbsp;</div>
                        <!--<div style="display:table-cell;" class="button" data-path="thepaper">&nbsp;&nbsp;CALL&nbsp;FOR&nbsp;PAPERS&nbsp;&nbsp;</div>&nbsp;&nbsp;-->
                        
                    </div>
                    <div class="navRow" id="myNavRow">                    
                        <!--<div style="display:table-cell;" class="button" data-path="thetravel">&nbsp;&nbsp;TRAVEL&nbsp;GRANT&nbsp;&nbsp;</div>&nbsp;&nbsp;-->
                        <!--<div style="display:table-cell;"  class="button" data-path="thedates">&nbsp;&nbsp;DATES&nbsp;&nbsp;</div>&nbsp;-->
                        <div style="display:table-cell;" class="button" data-path="thedemos">&nbsp;&nbsp;DEMOS&nbsp;&nbsp;</div>&nbsp;&nbsp;                        
                        <div style="display:table-cell;" class="button" data-path="theinfo">&nbsp;&nbsp;SPEAKER&nbsp;INFO&nbsp;&nbsp;</div>
                        <div style="display:table-cell;" class="button" data-path="theorg">&nbsp;&nbsp;ORGANIZERS&nbsp;&nbsp;</div>
                    </div>
                </div>
            </div>
        </div>
        
        
        <div id="container" class="content">
            <div id="thetop" class="topBackground backBanner">
                    <!--<img src = "./images/back.jpg"/>-->
            </div>
            <div style="background-color: #CCCCCC; z-index: 10; position: fixed; left: 10px; padding: 10px; border-radius: 10px;" class="button" data-path="thetop">
                Top
            </div>
            
            <div id="theabout" class="projectItem">
                <div class="shrunkLeft">
                <p><h1>HUBEDA</h1></p>

				<p><h2>Workshop on Human Behavior Data Acquisition for Human-Robot Interaction</h2></p>

                <p><b>Co-Located with IEEE RO-MAN 2024<br/>Workshop Date: August 30, 2024<br/>
                Workshop Venue: Westin Pasadena, 191 N Los Robles Ave, Pasadena, CA 91101</b></p>
                <!--<p><b><div class="inlink" data-path="thepaper">Call for Papers</div></b></p>-->
                
                <p>A vast number of advancements have been made in making robots engage in more natural human-robot interaction (HRI). To close the gap on enabling safe seamless human-aware HRI, collection of large data on human behavior, specially diverse multi-person interactions spanning large participant counts, is still an open challenge. Data on human behavior spans visual information on human movements, gestures, and interactions, spoken content, textual information, and physiological data such as heart rate, electromyography, and electrodermal activity.</p>

                <p>Through <b>invited talks, demos, papers, and posters,</b> HUBEDA addresses fundamental questions on the acquisition, analysis, and use of large-scale data on human behavior and interactions to enable data-informed HRI. Following topics are of interest.
                <ul>
                    <li>Datasets on single-person behavior and multi-person interaction for HRI</li>
                    <li>Multimodal setups integrating visual, audio, inertial, tactile, and physiological sensing for full-range capture</li>
                    <li>Augmented reality / virtual reality (AR/VR) approaches to scale up human subject data collection for HRI</li>
                    <li>AI, machine learning (ML), vision, and natural language processing (NLP) approaches to estimate parameters of interest for HRI from datasets on human behavior</li>
                    <li>Leveraging large vision and language models to decipher human activity, text, and spoken content</li>
                    <li>Robotic implementations driven using human behavior datasets</li>
                    <li>Robot learning from human behavior datasets and multi-person interactions</li>
                    <li>Applications involving multiple agents such as collaborative assembly, handover, repair</li>
                    <li>Ethical practices in data collection for single/multi-person behavior</li>
                    <li>Addressing challenges in reaching out to diverse population groups, e.g., children, older adults, and individuals with disabilities, to scale up data collection</li>
                </ul>
                </p>
                 
                <p>The workshop is supported through a generous donation from the <a href="https://delucafoundation.org/">De Luca Foundation</a>.</p>
                </div>
            </div>

            <div id="schedule" class="projectItem2">
                <div class="shrunkLeft">
                <p><h2>PROGRAM SCHEDULE</h2>
                <ul>
                    <li>9:00am - 9:10am: Opening Remarks (HUBEDA organizers: Natasha Banerjee, Maria Kyrarini, Sean Banerjee)</li>
                    <li>9:15am - 10:15am: Talk by Arash Ajoudani</li>
                    <li>10:15am - 10:55am: Paper Presentations and Robotics Demo from TARS
                        <ul>
                            <li>Analysis of Power Consumption Data Sets for Human-Robot Interaction Using Knowledge Graph and eXtended Reality by Ru-Guan Wang, Chu Hsien Tsai, Mei Cheng Tseng, Chi-Yun Ke, Yi-Fan Chien, Mei-Ling Chuang, and Chien-Cheng Chou</li>
                            <li>VR to AR 3D Display: Enhancing Remote Tele-operation in Healthcare with User-Friendly Interface by Himanshu Vishwakarma and Pradipta Biswas</li>
                            <li>A preliminary assessment of the detection of ERPs with visual stimuli in HRI task by Alessandra Fava, Valeria Villani and Lorenzo Sabattini</li>
                            <li>Toward an Ethical Framework for Using AI in the Assessment of Emotions and Other Mental States by Max Parks and Mark Allison</li>
                            <li>Systematic Enhancement of Neural Sensory Equipment (SENSE) of Tactile Feedback for Assistive Robots by Mashrur Wasek, Nathan Park, Jungkwon Kim and Chung Hyuk Park</li>
                            <li>Robotics Demo from TARS by Ava Megyeri</li>
                        </ul>
                    </li>
                    <li>11:00am - 12:00pm: Robotics Demo from Universal Robots by Pat Uetz and Rafael Mancilla</li>
                    <li>12:00am - 1:00pm: Lunch and Farewell (organized by RO-MAN)</li>
                    <li>1:00pm - 2:00pm: Talk by Elaine Short</li>
                    <li>2:00pm - 3:00pm: Talk by Vesna Novak</li>
                    <li>3:00pm - 3:10pm: Coffee Break</li>
                    <li>3:10pm - 4:10pm: Talk by Paul Robinette</li>
                    <li>4:00pm - 5:10pm: Talk by Pradipta Biswas</li>
                    <li>5:10pm - 5:15pm: Closing Remarks (HUBEDA organizers: Natasha Banerjee, Maria Kyrarini, Sean Banerjee)</li>
                </ul>
                </p>
                </div>
            </div>

            <div id="theinvited" class="projectItem2">
                <div class="shrunkLeft">
                <p><h2>INVITED SPEAKERS</h2>
                    <div class="peopleTable">
                        <div class="peopleCell">
                            <div class="peopleContainer">
                                <img src="images/aajoudani.jpg"></img>
                            </div>
                            <div class="peopleBio">
                                <a target = "_blank" href="https://www.iit.it/people-details/-/people/arash-ajoudani">ARASH AJOUDANI</a><br/>Director, HRII Laboratory<br/>Italian Institute of Technology<br/>
                            </div>
                        </div>
                        <div class="peopleCell">
                            <div class="peopleContainer">
                                <img src="images/vnovak.jpg"></img>
                            </div>
                            <div class="peopleBio">
                                <a target = "_blank" href="https://researchdirectory.uc.edu/p/novakdn">VESNA NOVAK</a><br/>Associate Professor, Department of Electrical Engineering and Computer Science<br/>University of Cincinnati<br/>
                            </div>
                        </div>
                        <div class="peopleCell">
                            <div class="peopleContainer">
                                <img src="images/eshort.jpeg"/>
                            </div>
                            <div class="peopleBio">
                                <a target = "_blank" href="https://eshort.github.io/">ELAINE SHORT</a><br/>Assistant Professor, Department of Computer Science<br/>Tufts University<br/>
                            </div>
                        </div>
                        <div class="peopleCell">
                            <div class="peopleContainer">
                                <img src="images/probinette.jpeg"></img>
                            </div>
                            <div class="peopleBio">
                                <a target = "_blank" href="https://sites.uml.edu/paul-robinette/">PAUL ROBINETTE</a><br/>Assistant Professor, Department of Electrical and Computer Engineering<br/>University of Massachusetts Lowell<br/>
                            </div>
                        </div>
                        <div class="peopleCell">
                            <div class="peopleContainer">
                                <img src="images/pbiswas.jpg"></img>
                            </div>
                            <div class="peopleBio">
                                <a target = "_blank" href="https://cambum.net/PB/">PRADIPTA BISWAS</a><br/>Associate Professor, Center for Product Design and Manufacturing<br/>Indian Institute of Science<br/>
                            </div>
                        </div>                    
                    </div>
                    
                </p>                
            </div>

            <div id="thedemos" class="projectItem2">
                <div class="shrunkLeft">
                    <p><h2>DEMOS</h2>

                    <p>At HUBEDA, we will have <b>demos</b> from <a href="https://www.universal-robots.com/">Universal Robots</a> on current advances and from <a href="https://tars-home.github.io/">TARS</a> on integrating physiological sensing with robotic to enable HRI applications. Attendees will receive the opportunity to interact with robots from UR and TARS, and ask the presenters questions about the technology.</p>

                    <p><h3>DEMO PRESENTERS</h3></p>
                    
                    <div class="peopleTable">
                        
                        <div class="peopleCell">
                            <div class="peopleContainer">
                                <img src="images/rmancilla.jpg"></img>
                            </div>
                            <div class="peopleBio">
                                RAFAEL MANCILLA<br/>Business Development Manager<br/>Universal Robots<br/>
                            </div>
                        </div>
                        <div class="peopleCell">
                            <div class="peopleContainer">
                                <img src="images/puetz.jpg"></img>
                            </div>
                            <div class="peopleBio">
                                PAT UETZ<br/>Business Development Manager<br/>Advanced Technology Consultants<br/>
                            </div>
                        </div>
                        <div class="peopleCell">
                            <div class="peopleContainer">
                                <img src="images/amegyeri.jpg"></img>
                            </div>
                            <div class="peopleBio">
                                AVA MEGYERI<br/>Graduate Student, Department of Computer Science and Engineering<br/>Wright State University<br/>
                            </div>
                        </div>
                    </div>
                </p>                
            </div>

            
            <!--<div id="thepaper" class="projectItem2">
                <div class="shrunkLeft">
                <p><h2>CALL FOR PAPERS</h2>
                We invite authors to submit papers of up to 6 pages (including references) on the <span class="inlink" data-path="theabout">topics of interest</span>. Accepted papers will be published on the workshop webpage and on arXiv. Authors are also invited to submit extended abstracts of up to 4 pages (including references) to be presented at the workshop on early-phase work, late-breaking results, and demonstrations, to be presented at the workshop.  Authors of accepted papers will be invited to submit a full paper a special issue (to be determined) to enable publications associated with the workshop.</p>

                <p>The goal of the workshop is to bootstrap research on data-driven methods in HRI. A full working result on a robot, though encouraged, is not strictly necessary for acceptance. Rather, papers must demonstrate how their setup, methods, algorithms, and/or datasets provide human behavior data to HRI and/or robotics in general.</p>

                <p><b>Submission link:</b> <a target="_blank" href="https://easychair.org/my/conference?conf=hubeda2024">https://easychair.org/my/conference?conf=hubeda2024</a>

                    <p>Papers submissions should follow the template at <a target="_blank" href="https://www.ro-man2024.org/contributing/regularpapersubmission">https://www.ro-man2024.org/contributing/regularpapersubmission</a>.</p>
                </div>
            </div>-->
            <!--<div id="thetravel" class="projectItem2">
                <div class="shrunkLeft">
                <p><h2>TRAVEL GRANT</h2>
                To encourage student participation, HUBEDA will be sponsoring <b>travel grants</b> for student attendees based on merit and need. Travel grants are competitive, and priority will be provided to presenters. We highly encourage students from underrepresented communities, including women, individuals with disabilities, and students from underrepresented minorities to apply. The link for the travel grant will be posted soon.</p>
                </div>
            </div>-->

            


            <!--<div id="thedates" class="projectItem2">
                <div class="shrunkLeft">
                <p><h2>IMPORTANT DATES</h2>
                <ul>
                    <li>Paper Submission Deadline (Second Extension): <s>Jun 30, 2024, AOE</s> <s>Jul 15, 2024, AOE</s> <b>Jul 31, 2024, AOE</b></li>
                    <li>Notification of Paper Acceptance: Aug 3, 2024, AOE</li>
                    <li>Travel Grant Application Submission: Aug 5, 2024, AOE</li>
                    <li>Final Paper Submission: Aug 7, 2024, AOE</li>
                    <li>Travel Grant Notification: Aug 7, 2024, AOE</li>
                    <li>Workshop: Aug 30, 2024</li>
                </ul>
                </p>
                </div>
            </div>-->

            <div id="theinfo" class="projectItem2">
                <div class="shrunkLeft">
                <p><h2>SPEAKER INFO</h2>
                    
                    <div style="text-align: center;">
                        <h3><a target = "_blank" href="https://www.iit.it/people-details/-/people/arash-ajoudani">ARASH AJOUDANI</a></h3>
                    </div>
                    <div class="peopleContainer2">
                        <img src="images/aajoudani.jpg"></img>
                    </div>
                    <br/>Title: TBA<br/><br/>

                        Abstract: TBA<br/><br/>

                        Bio: Arash Ajoudani is the director of the Human-Robot Interfaces and Interaction (HRI²) laboratory at IIT. He also coordinates the Robotics for Manufacturing (R4M) lab of the Leonardo labs, and is a principal investigator of the IIT-Intellimech JOiiNT lab. He is a recipient of the European Research Council (ERC) proof-of-concept grant 2023 Real-Move and the ERC starting grant 2019 (Ergo-Lean), the coordinator of the Horizon-2020 project SOPHIA, the co-coordinator of the Horizon-2020 project CONCERT, and a principal investigator of the HORIZON-MSCA project RAICAM, and the national projects LABORIUS, COROMAN, and ReFinger.  He is a recipient of the IEEE Robotics and Automation Society (RAS) Early Career Award 2021, and winner of the SmartCup Liguria award 2023, Amazon Research Awards 2019, of the Solution Award 2019 (MECSPE2019), of the KUKA Innovation Award 2018, of the WeRob best poster award 2018, and of the best student paper award at ROBIO 2013. His PhD thesis was a finalist for the Georges Giralt PhD award 2015 - best European PhD thesis in robotics. He was also a finalist for the best paper award on human-robot interaction at ICRA2024, the best paper award mobile manipulation at IROS 2022, the best paper award at Humanoids 2022 (oral category), the Solution Award 2020 (MECSPE2020), the best conference paper award at Humanoids 2018, the best interactive paper award at Humanoids 2016, the best oral presentation award at Automatica (SIDRA) 2014, and for the best manipulation paper award at ICRA 2012. His main research interests are in physical human-robot interaction, mobile manipulation, robust and adaptive control, assistive robotics, and tele-robotics.
                    <br/><br/><br/>
                    
                    
                    <div style="text-align: center;">
                        <h3><a target = "_blank" href="https://researchdirectory.uc.edu/p/novakdn">VESNA NOVAK</a></h3>
                    </div>
                    <div class="peopleContainer2">
                        <img src="images/vnovak.jpg"></img>
                    </div>
                    
                        <br/>Title: Measuring human behavior in robotic rehabilitation gyms: from simulation to real-world studies<br/><br/>

                        Abstract: As the cost of rehabilitation robots decreases, it is increasingly common for rehabilitation clinics to own multiple robots. Patients can use these robots to exercise together in a group setting, tentatively dubbed a robotic rehabilitation gym. Such gyms have significant potential as a new rehabilitation delivery model, as they could both increase patient motivation through social interaction and increase cost-effectiveness by reducing the number of therapists needed per patient. However, such gyms may benefit from additional software support (e.g., group performance monitoring, patient-robot assignment suggestions) to avoid overloading the supervising therapist and allow them to focus on the most critical aspects of therapy. However, very little data exists on how humans behave in such gyms, limiting further development. In this presentation, I will thus discuss our ongoing work on simplified simulation models of robotic rehabilitation gyms, which allowed us to develop and evaluate software such as intelligent dynamic patient-robot assignment via stochastic optimization or learning from a human expert. I will then discuss possible next steps for actual human subjects studies of such robotic gyms.<br/><br/>

                        Bio: Vesna Novak is an Associate Professor in the Department of Electrical and Computer Engineering at the University of Cincinnati, with secondary appointments in biomedical engineering and computer science. She received her diploma and PhD in electrical engineering from the University of Ljubljana in 2008 and 2011, respectively. She was a postdoctoral fellow at ETH Zurich, Switzerland, from 2012 to 2014, and then an Assistant and Associate Professor at the University of Wyoming from 2014 to 2021. She is currently Principal Investigator for two NSF grants and one NIH grant, and has authored 49 peer-reviewed journal papers as well as over 50 peer-reviewed conference papers, with over 3500 total citations per Google Scholar. Her research interests include rehabilitation robotics, wearable robotics, serious games, affective computing, and human activity recognition.<br/><br/><br/>
                    

                    <div style="text-align: center;">
                        <h3><a target = "_blank" href="https://eshort.github.io/">ELAINE SHORT</a></h3>
                    </div>
                    <div class="peopleContainer2">
                        <img src="images/eshort.jpeg"/>
                    </div>
                    
                        <br/>Title:  Human-Centered AI for Accessible and Assistive Robotics: Towards a Disability-Centered HRI<br/><br/>

                        Abstract:  Powered by advances in AI, especially machine learning, robots are becoming smarter and more widely used.  Robots can provide critical assistance to people in a variety of contexts, from improving the efficiency of workers to helping people with disabilities in their day-to-day lives.  However, inadequate attention to the needs of users in developing these intelligent robots results in systems that are both less effective at their core tasks and more likely to do unintended harm.  The Assistive Agent Behavior and Learning (AABL) Lab at Tufts University seeks to apply human-centered design thinking, especially disability ethics, to the design of state-of-the-art robot learning algorithms and interaction frameworks.  This talk will explore how disability-community-centered thinking can be used to inspire new directions for intelligent interactive robotics and review recent work from the AABL lab at the intersection of assistive robotics, robot learning, and human-robot interaction.<br/><br/>

                        Bio: Elaine Schaertl Short is the Clare Boothe Luce Assistant Professor of Computer Science at Tufts University. She holds a PhD and MS in Computer Science at the University of Southern California (USC) and a BS in Computer Science from Yale University. Her research applies human-centered design and disability community values to the development, deployment, and evaluation of AI and machine learning for robotics, including: human-centered human-in-the-loop machine learning; disability-friendly assistive robotics; autonomous HRI in groups, public spaces, and other human-human contexts; and accessibility and disability inclusion in robotics education and the computing research community.  She is as committed to human-centered research practices as she is to algorithm and robot design: her work spans from designing a low-cost open-source open-hardware robot platform, to understanding family group interactions with socially assistive robots, to designing new neural network architectures for improving human-in-the-loop robot learning.   As a disabled faculty member, Elaine is particularly passionate about disability rights in her service work.  She is a co-PI of AccessComputing and co-Chair of AccessSIGCHI, an advocacy group that works to increase the accessibility of the 24 SIGCHI conferences.<br/><br/><br/>
                    

                    <div style="text-align: center;">
                        <h3><a target = "_blank" href="https://sites.uml.edu/paul-robinette/">PAUL ROBINETTE</a></h3>
                    </div>
                    <div class="peopleContainer2">
                        <img src="images/probinette.jpeg"></img>
                    </div>
                    
                        <br/>Title: Collecting Data to Build Trustworthy, Fieldable Robotics<br/><br/>

                        Abstract: As robots enter everyday life, it is important to understand how and why people trust them. Too much trust leads to over-reliance and the expectation that a system performs flawlessly. Too little trust causes people to under-utilize robots, reducing the effectiveness of a deployed system. This talk will discuss data collection experiences, including some cautionary tales, over the last decade of human-robot trust experiments in multiple domains.<br/><br/>

                        Bio: Paul Robinette is an assistant professor in the Department of Electrical and Computer Engineering at the University of Massachusetts Lowell (UML). He has performed extensive experiments on human-robot trust in time-critical situations in virtual simulations, the lab, and the field. In recent years, he focused on field robotics: in-situ human-robot teaming experiments in the marine domain and field experiments for river navigation of autonomous surface vehicles. These projects have yielded datasets that have been released for the human-robot interaction community and the marine robotics community.<br/><br/><br/>
                    

                    <div style="text-align: center;">
                        <h3><a target = "_blank" href="https://cambum.net/PB/">PRADIPTA BISWAS</a></h3>
                    </div>
                    <div class="peopleContainer2">
                        <img src="images/pbiswas.jpg"></img>
                    </div>
                    
                        <br/>Title: AR, VR, MR – How Does It Matter<br/><br/>

                        Abstract: The Covid 19 pandemic and associated work-from-home culture reignites the importance of immersive media. Major industries used various terms like Metaverse (Facebook), Mesh (Microsoft), Nth Floor (Accesnture) and so on to invest and commercialize immersive media related products. Traditionally, immersive media is described through a continuum between reality and virtual reality along with intermediate systems known as Augmented and Mixed Reality systems. In the middle of the commercialization efforts and advertisements from software giants, end users are often confused about the best solution for their needs. There are not many studies on analyzing end users’ feedback across the continuum of immersive media. This talk will present analysis on ocular parameters and EEG power bands of different brain regions while users undertook pointing and selection tasks using state-of-the-art AR, MR and VR media.  In parallel, novel case studies will be presented from Aerospace, Smart Manufacturing and Assistive Technology involving XR technologies. The talk will be accompanied by a plethora of video and live demonstrations of XR technology.<br/><br/>

                        Bio: Pradipta Biswas is an Associate Professor at the Department of Design and Manufacturing and associate faculty at the Robert Bosch Centre for Cyber Physical Systems of Indian Institute of Science. He has been elected as a vice chairman of ITU Study Group 9 and also a Co-Chair of the IRG AVA  at International Telecommunication Union. His research focuses on user modelling and multimodal human-machine interaction for aviation and automotive environments and for assistive technology. I set up and lead the Interaction Design (I3D) Lab. His research won Microsoft’s AI 4 Accessibility Grant Award, Facebook’s Responsible AR/VR Award and research grants from Collins Aerospace, Siemens, British Telecom (on AR/VR Systems),  Faurecia Groupe Services Ltd. (on Automotive UI), Wipro Ltd (On Autonomous Vehicle), Defence Research and Development Organization (DRDO), Departments of Science & Technology (DST) and BioTechnology (DBT) and was featured in New Scientist, NDTV, All India Radio and various other media outlets. Earlier, he was a Senior Research Associate at Engineering Department, Research Fellow at Wolfson College and Research Associate at Trinity Hall of University of Cambridge. He completed PhD in Computer Science at  University of Cambridge Computer Laboratory and Trinity College in 2010 and was awarded a Gates-Cambridge Scholarship in 2006. He is a member of the UKRI International Development Peer Review College, Association of Computing Machinery (ACM) and was a professional member of the Society of Flight Test Engineers British Computer Society, Associate Fellow at the UK Higher Education Academy and Royal Society of Medicine.
                        <br/><br/><br/>    

                        <div style="text-align: center;">
                        <h3>RAFAEL MANCILLA</h3>
                    </div>
                    <div class="peopleContainer2">
                        <img src="images/rmancilla.jpg"></img>
                    </div>
                    
                        <br/>Bio: Rafael Mancilla is Business Development Manager at Universal Robots. He spent four years as an application engineer ensuring customers in the West region had successful deployments prior to moving into sales. He works daily with partners and customers to vet out and assist in cobot implementation and is a Certified Universal Robots Instructor. Prior to this role, Rafael worked as a controls engineer on power generating systems, and he obtained his Bachelor of Science degree in Computer Engineering from Cal Poly, Pomona.<br/><br/><br/>
                    
                    
                    <div style="text-align: center;">
                        <h3>PAT UETZ</h3>
                    </div>
                    <div class="peopleContainer2">
                        <img src="images/puetz.jpg"></img>
                    </div>
                    
                        <br/>Bio: Pat graduated from Western Michigan University, with a degree in Business Marketing.  After spending the first two years of his career for ATC in Michigan, Pat and his family relocated to Southern California in 2012.  With 14 years of experience in Career and Technical Education, Pat is passionate about providing educators with the latest equipment and curriculum related to Advanced Manufacturing and Automated Technologies.<br/><br/><br/>                
                </p>                
            </div>

            <div id="theorg" class="projectItem2">
                <div class="shrunkLeft">
                <p><h2>ORGANIZERS</h2>
                <div class="peopleTable">
                    <div class="peopleCell">
                        <div class="peopleContainer">
                            <img src="images/nbanerjee.jpg"></img>
                        </div>
                        <div class="peopleBio">
                            <a target = "_blank" href="http://tars-home.github.io/">NATASHA BANERJEE</a><br/>Associate Professor<br/>Terascale All-sensing Research Studio (TARS)<br/>Wright State University<br/>
                        </div>
                    </div>
                    <div class="peopleCell">
                        <div class="peopleContainer">
                            <img src="images/mkyrarini.jpg"></img>
                        </div>
                        <div class="peopleBio">
                            <a target = "_blank" href="https://sites.google.com/view/mariakyrarini/home">MARIA KYRARINI</a><br/>Assistant Professor<br/>Human-Machine Interaction &amp; Innovation Lab (HMI2)<br/>Santa Clara University<br/>
                        </div>
                    </div>
                    <div class="peopleCell">
                        <div class="peopleContainer">
                            <img src="images/sbanerjee.jpg"/>
                        </div>
                        <div class="peopleBio">
                            <a target = "_blank" href="http://tars-home.github.io/">SEAN BANERJEE</a><br/>Associate Professor<br/>Terascale All-sensing Research Studio (TARS)<br/>Wright State University<br/>
                        </div>
                    </div>                    
                </div>
                For questions, contact us as hubeda2024 a gmail d com<br/><br/><br/><br/><br/>
            </div>
            
    
            <script type="text/javascript">
                $(".button").on("click", function(){
                    var path = $(this).attr("data-path");
                    var anchor = $("#" + path);
                    var position = anchor.position().top + $("#container").scrollTop();                
                    $("#container").animate({scrollTop: position});
                });
                $(".inlink").on("click", function(){
                    var path = $(this).attr("data-path");
                    var anchor = $("#" + path);
                    var position = anchor.position().top + $("#container").scrollTop();
                    $("#container").animate({scrollTop: position});
                });
                
                
                var interval;
                var userScroll = false;
                
                function mouseEvent(e) {
                    userScroll = true;
                }            
            </script>
        </div>
    </BODY>
</HTML>

